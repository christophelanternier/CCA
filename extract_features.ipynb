{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to extract features from the neural network for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophelanternier/anaconda/lib/python2.7/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#general imports\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import pandas as pd\n",
    "from datetime import datetime, time, timedelta\n",
    "import tables\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import time, datetime, timedelta\n",
    "\n",
    "#text imports\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()\n",
    "import random\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "#images imports\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import common.p_vgg16 as p_vgg16\n",
    "import common.p_cca as p_cca\n",
    "import common.p_w2v as p_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get all usefull paths\n",
    "\n",
    "homePath = os.path.join(os.path.expanduser('~'), 'Documents', 'Cours', 'MVA', 'OBJR', 'FinalProject')\n",
    "MODEL_PATH = os.path.join(homePath, 'model')+os.sep\n",
    "IMAGES_PATH = os.path.join(homePath, 'images', 'train2014')+os.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize feature exctraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.15s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "#Load pre-trained word2vec NN, takes a bit of time\n",
    "model = gensim.models.Word2Vec.load_word2vec_format(MODEL_PATH+'GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "# initialize COCO api for caption annotations\n",
    "dataType='train2014'\n",
    "annFile = '%s/annotations/captions_%s.json'%(homePath,dataType)\n",
    "coco_caps=COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = p_w2v.W2V(model, coco_caps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "26 fc6_W (25088, 4096)\n",
      "27 fc6_b (4096,)\n",
      "28 fc7_W (4096, 4096)\n",
      "29 fc7_b (4096,)\n",
      "30 fc8_W (4096, 1000)\n",
      "31 fc8_b (1000,)\n"
     ]
    }
   ],
   "source": [
    "#initialize CNN\n",
    "sess = tf.Session()\n",
    "imgs = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "vgg = p_vgg16.VGG16(imgs, MODEL_PATH + 'vgg16_weights.npz', sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Choose the number of samples you want in your data\n",
    "nsamples = 82782\n",
    "imgIds = coco_caps.getImgIds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.567808\n",
      "0\n",
      "0:00:55.444678\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#This part allows to save the data by batches of 10000 (allows not to lose everything in case of a buggf)\n",
    "time1 = datetime.now()\n",
    "\n",
    "text_data = np.zeros((10000, 300))\n",
    "image_data = np.zeros((10000, 4096))\n",
    "save = True\n",
    "\n",
    "for i in range(nsamples+1):\n",
    "    #Text Features\n",
    "    j = i%10000\n",
    "    img = coco_caps.loadImgs(imgIds[i])[0]\n",
    "    text_data[j,:] = w2v.get_text_representations(imgIds[i])\n",
    "\n",
    "    #image Features\n",
    "    img1 = imread(IMAGES_PATH+img['file_name'], mode='RGB')\n",
    "    img1 = imresize(img1, (224, 224))\n",
    "    prob = sess.run(vgg.fc1, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    # To try other fc layers in the NN:\n",
    "    #prob = sess.run(vgg.fc2, feed_dict={vgg.imgs: [img1]})[0]\n",
    "    image_data[j, :] = prob\n",
    "\n",
    "    if i%100 ==0:\n",
    "        time2 = datetime.now()\n",
    "        td = time2-time1\n",
    "        print(td)\n",
    "        print str(i)\n",
    "    \n",
    "    if (i+1)%10000 == 0 and save == True:\n",
    "        hdf5_path = \"data/data_fc1_part_\"+str((i+1)//10000)+\".hdf5\"\n",
    "        hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "        text_storage = hdf5_file.create_array(hdf5_file.root, 'text', text_data)\n",
    "        image_storage = hdf5_file.create_array(hdf5_file.root, 'image', image_data)\n",
    "        hdf5_file.close()\n",
    "        print(\"PART \"+str(i//10000)+\" done\")\n",
    "        text_data = np.zeros((10000, 300))\n",
    "        image_data = np.zeros((10000, 4096))\n",
    "            \n",
    "#np.savetxt('data/text_data_'+str(layers_string[k])+'.txt', text_data)\n",
    "#np.savetxt('data/image_data_'+str(layers_string[k])+'.txt', image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_data = image_data[:2783]\n",
    "text_data = text_data[:2783]\n",
    "\n",
    "hdf5_path = \"data/data_fc1_part_9.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "text_storage = hdf5_file.create_array(hdf5_file.root, 'text', text_data)\n",
    "image_storage = hdf5_file.create_array(hdf5_file.root, 'image', image_data)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejoin various batches of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = np.zeros((82783, 300))\n",
    "image_data = np.zeros((82783, 4096))\n",
    "for i in range(1,9):\n",
    "    #hdf5_path = \"data/data_fc2_part_1.hdf5\"\n",
    "    hdf5_path = \"data/data_fc1_part_\"+str(i)+\".hdf5\"\n",
    "    read_hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "    # Here we slice [:] all the data back into memory, then operate on it\n",
    "    text_data[(i-1)*10000:i*10000, :] = read_hdf5_file.root.text[:]\n",
    "    image_data[(i-1)*10000:i*10000, :] = read_hdf5_file.root.image[:]\n",
    "\n",
    "    read_hdf5_file.close()\n",
    "\n",
    "hdf5_path = \"data/data_fc1_part_9.hdf5\"\n",
    "read_hdf5_file = tables.open_file(hdf5_path, mode='r')\n",
    "# Here we slice [:] all the data back into memory, then operate on it\n",
    "text_data[80000:82783, :] = read_hdf5_file.root.text[:]\n",
    "image_data[80000:82783, :] = read_hdf5_file.root.image[:]\n",
    "\n",
    "read_hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"data/data_fc1_all.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "text_storage = hdf5_file.create_array(hdf5_file.root, 'text', text_data)\n",
    "image_storage = hdf5_file.create_array(hdf5_file.root, 'image', image_data)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Category features for 3 view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_cats = []\n",
    "cat_data = np.zeros((82783, 300))\n",
    "image_without_cat = []\n",
    "for i in range(82783):\n",
    "    img = coco_caps.loadImgs(imgIds[i])[0]\n",
    "    try:\n",
    "        cat = dic_img_cat[img['id']]\n",
    "        if len(cat.split(' ')) > 1:\n",
    "            cat_array = []\n",
    "            for word in cat.split(' '):\n",
    "                cat_array.append(model[word])\n",
    "            cat_data[i, :] = sum(cat_array)/len(cat_array)\n",
    "        else:   \n",
    "            cat_data[i, :] = model[cat]\n",
    "    except:\n",
    "        image_without_cat.append(i)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Glove features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print \"Loading Glove Model\"\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = [float(val) for val in splitLine[1:]]\n",
    "        model[word] = embedding\n",
    "    print \"Done.\",len(model),\" words loaded!\"\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "model = loadGloveModel('data/glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = p_w2v.W2V(model, coco_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time1 = datetime.now()\n",
    "\n",
    "text_data = np.zeros((nsamples+1, 300))\n",
    "\n",
    "for i in range(nsamples+1):\n",
    "    #Text Features\n",
    "    text_data[i,:] = w2v.get_text_representations(imgIds[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdf5_path = \"data/data_glove.hdf5\"\n",
    "hdf5_file = tables.open_file(hdf5_path, mode='w')\n",
    "text_storage = hdf5_file.create_array(hdf5_file.root, 'text', text_data)\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imgId = imgIds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'People', u'shopping', u'in', u'an', u'open', u'market', u'for', u'vegetables']\n"
     ]
    }
   ],
   "source": [
    "annIds = coco_caps.getAnnIds(imgIds=imgId);\n",
    "anns = coco_caps.loadAnns(annIds)\n",
    "# Compute annotations mean (text word representation for one image)\n",
    "global_mean = False\n",
    "caption_vectors_mean = np.zeros(300)\n",
    "\n",
    "caption_vectors = []\n",
    "print anns[0]['caption'][:-1].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caption_vectors = []\n",
    "for word in anns[0]['caption'][:-1].split(' '):\n",
    "    caption_vectors.append(np.array(model[word.lower()]))\n",
    "    caption_vectors_mean += sum(caption_vectors)/len(caption_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
